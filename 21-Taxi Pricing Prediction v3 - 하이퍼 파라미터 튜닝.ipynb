{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/05 14:02:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "MAX_MEMORY=\"5g\"\n",
    "spark = SparkSession.builder.appName(\"taxi-fare-prediciton\")\\\n",
    "                .config(\"spark.executor.memory\", MAX_MEMORY)\\\n",
    "                .config(\"spark.driver.memory\", MAX_MEMORY)\\\n",
    "                .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_dir = \"/home/ubuntu/working/spark-examples/data/ml_data_taxi\" # 로컬 디렉토리로 지정. hdfs를 사용 할 수도 있다!\n",
    "\n",
    "train_df = spark.read.parquet(f\"{data_dir}/train/\")\n",
    "test_df  = spark.read.parquet(f\"{data_dir}/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_8de26289538f,\n",
       " OneHotEncoder_a2b8abb24f26,\n",
       " StringIndexer_f63fb8bc8813,\n",
       " OneHotEncoder_0cbde98fb496,\n",
       " StringIndexer_bd07ebed515b,\n",
       " OneHotEncoder_35d56cb78c6c]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stages = []\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "# OneHotEncoding을 수행할 컬럼을 지정\n",
    "cat_features = [\n",
    "    \"pickup_location_id\",\n",
    "    \"dropoff_location_id\",\n",
    "    \"day_of_week\"\n",
    "]\n",
    "\n",
    "for c in cat_features:\n",
    "    # 1. 데이터를 문자열 형식으로 바꿔준다. setHandleInvalid : Null값 같은 데이터를 어떻게 처리 할건지\n",
    "    cat_indexer = StringIndexer(inputCol=c, outputCol=c+\"_idx\").setHandleInvalid(\"keep\")\n",
    "    \n",
    "    # 2. One Hot Encoding 수행\n",
    "    onehot_encoder = OneHotEncoder(inputCols=[cat_indexer.getOutputCol()], outputCols=[c+\"_onehot\"])\n",
    "    \n",
    "    stages += [cat_indexer, onehot_encoder]\n",
    "\n",
    "stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_8de26289538f,\n",
       " OneHotEncoder_a2b8abb24f26,\n",
       " StringIndexer_f63fb8bc8813,\n",
       " OneHotEncoder_0cbde98fb496,\n",
       " StringIndexer_bd07ebed515b,\n",
       " OneHotEncoder_35d56cb78c6c,\n",
       " VectorAssembler_60464bdd3272,\n",
       " StandardScaler_6632593e6ea4,\n",
       " VectorAssembler_b7a09a801998,\n",
       " StandardScaler_3c96c5bd2bce,\n",
       " VectorAssembler_e0010bfe3fa8,\n",
       " StandardScaler_842e7df723c5]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "num_features = [\n",
    "    \"passenger_count\",\n",
    "    \"trip_distance\",\n",
    "    \"pickup_time\"\n",
    "]\n",
    "\n",
    "for n in num_features:\n",
    "    \n",
    "    # 각각의 컬럼의 데이터가 벡터화. ex) 1.5 -> [1.5]\n",
    "    num_assembler = VectorAssembler(inputCols=[n], outputCol=n+\"_vector\")\n",
    "    \n",
    "    # StandardScaling 수행\n",
    "    num_scaler = StandardScaler(inputCol=num_assembler.getOutputCol(), outputCol=n+\"_scaled\")\n",
    "    \n",
    "    stages += [num_assembler, num_scaler]\n",
    "\n",
    "stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_8de26289538f,\n",
       " OneHotEncoder_a2b8abb24f26,\n",
       " StringIndexer_f63fb8bc8813,\n",
       " OneHotEncoder_0cbde98fb496,\n",
       " StringIndexer_bd07ebed515b,\n",
       " OneHotEncoder_35d56cb78c6c,\n",
       " VectorAssembler_60464bdd3272,\n",
       " StandardScaler_6632593e6ea4,\n",
       " VectorAssembler_b7a09a801998,\n",
       " StandardScaler_3c96c5bd2bce,\n",
       " VectorAssembler_e0010bfe3fa8,\n",
       " StandardScaler_842e7df723c5,\n",
       " VectorAssembler_5f623cce4331]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# _onehot이 붙은 컬럼과 _scaled 가 붙은 컬럼만 있으면 된다.\n",
    "assembler_inputs = [c + \"_onehot\" for c in cat_features] + [n + \"_scaled\" for n in num_features]\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"feature_vector\")\n",
    "stages += [assembler]\n",
    "\n",
    "stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 하이퍼 파라미터\n",
    "- 모델이 학습하지 못하는 파라미터\n",
    "- 사람이 직접 모델에 넣어주는 파라미터\n",
    "- 하이퍼 파라미터에 따라서 모델의 성능이 바뀔 수 있다.\n",
    "    - 적절한 최적의 파라미터를 찾아야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(\n",
    "    maxIter=50,\n",
    "    solver='normal',\n",
    "    labelCol=\"total_amount\",\n",
    "    featuresCol='feature_vector'\n",
    ")\n",
    "\n",
    "cv_stages = stages + [lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_8de26289538f,\n",
       " OneHotEncoder_a2b8abb24f26,\n",
       " StringIndexer_f63fb8bc8813,\n",
       " OneHotEncoder_0cbde98fb496,\n",
       " StringIndexer_bd07ebed515b,\n",
       " OneHotEncoder_35d56cb78c6c,\n",
       " VectorAssembler_60464bdd3272,\n",
       " StandardScaler_6632593e6ea4,\n",
       " VectorAssembler_b7a09a801998,\n",
       " StandardScaler_3c96c5bd2bce,\n",
       " VectorAssembler_e0010bfe3fa8,\n",
       " StandardScaler_842e7df723c5,\n",
       " VectorAssembler_5f623cce4331,\n",
       " LinearRegression_ad26a366029e]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline_79f2195c0218"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_pipeline = Pipeline(stages=cv_stages)\n",
    "cv_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch 및 CrossValidation 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossValidator_7dd7424170bd"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "param_grid = ParamGridBuilder()\\\n",
    "                .addGrid(lr.elasticNetParam, [0.1])\\\n",
    "                .addGrid(lr.regParam, [0.01])\\\n",
    "                .build()\n",
    "# param_grid = ParamGridBuilder()\\\n",
    "#                 .addGrid(lr.elasticNetParam, [0.1, 0.2, 0.3, 0.4, 0.5])\\\n",
    "#                 .addGrid(lr.regParam, [0.01, 0.02, 0.03, 0.04, 0.05])\\\n",
    "#                 .build()\n",
    "\n",
    "cross_val = CrossValidator(\n",
    "    estimator=cv_pipeline, # 파이프라인을 estimator로 사용하는 경우 제일 마지막 스테이지가 모델이어야 한다.\n",
    "    estimatorParamMaps=param_grid, # 설정하지 않으면 GridSearch 없이 Cross Validation만 진행\n",
    "    evaluator=RegressionEvaluator(labelCol=\"total_amount\"),\n",
    "    numFolds=5\n",
    ")\n",
    "\n",
    "cross_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- pickup_location_id: integer (nullable = true)\n",
      " |-- dropoff_location_id: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- pickup_time: integer (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 임의의 샘플 데이터 세트 만들기. 전체로 다 하면 시간이 많이 걸려서....\n",
    "toy_df = train_df.sample(False, 0.1, seed=1)\n",
    "toy_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2014:>               (0 + 2) / 2][Stage 2015:>               (0 + 0) / 2]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/spark-env-324/lib/python3.8/multiprocessing/pool.py:851\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 851\u001b[0m     item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_items\u001b[39m.\u001b[39;49mpopleft()\n\u001b[1;32m    852\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cv_model \u001b[39m=\u001b[39m cross_val\u001b[39m.\u001b[39;49mfit(toy_df)\n\u001b[1;32m      2\u001b[0m cv_model\n",
      "File \u001b[0;32m~/miniconda3/envs/spark-env-324/lib/python3.8/site-packages/pyspark/ml/base.py:161\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[1;32m    160\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[1;32m    162\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params))\n",
      "File \u001b[0;32m~/miniconda3/envs/spark-env-324/lib/python3.8/site-packages/pyspark/ml/tuning.py:689\u001b[0m, in \u001b[0;36mCrossValidator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    684\u001b[0m train \u001b[39m=\u001b[39m datasets[i][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mcache()\n\u001b[1;32m    686\u001b[0m tasks \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(\n\u001b[1;32m    687\u001b[0m     inheritable_thread_target,\n\u001b[1;32m    688\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam))\n\u001b[0;32m--> 689\u001b[0m \u001b[39mfor\u001b[39;00m j, metric, subModel \u001b[39min\u001b[39;00m pool\u001b[39m.\u001b[39mimap_unordered(\u001b[39mlambda\u001b[39;00m f: f(), tasks):\n\u001b[1;32m    690\u001b[0m     metrics[j] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (metric \u001b[39m/\u001b[39m nFolds)\n\u001b[1;32m    691\u001b[0m     \u001b[39mif\u001b[39;00m collectSubModelsParam:\n",
      "File \u001b[0;32m~/miniconda3/envs/spark-env-324/lib/python3.8/multiprocessing/pool.py:856\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    854\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pool \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    855\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    857\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    858\u001b[0m     item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_items\u001b[39m.\u001b[39mpopleft()\n",
      "File \u001b[0;32m~/miniconda3/envs/spark-env-324/lib/python3.8/threading.py:302\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    303\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2072:>                                                       (0 + 2) / 2]\r"
     ]
    }
   ],
   "source": [
    "cv_model = cross_val.fit(toy_df)\n",
    "cv_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BestModel 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m best_model \u001b[39m=\u001b[39m cv_model\u001b[39m.\u001b[39mbestModel\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cv_model' is not defined"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1974:>                                                       (0 + 2) / 2]\r"
     ]
    }
   ],
   "source": [
    "best_model = cv_model.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Parameter 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha = cv_model.bestModel.stages[-1]._java_obj.getElasticNetParam()\n",
    "best_reg_param = cv_model.bestModel.stages[-1]._java_obj.getRegParam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전체 데이터를 대상으로 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=stages) # 모델이 빠진 순수하게 전처리만 하는 파이프라인 생성\n",
    "fitted_transformer = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_train_df = fitted_transformer.transform(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best paramter로 모델 생성\n",
    "lr = LinearRegression(\n",
    "    maxIter=50,\n",
    "    solver='normal',\n",
    "    labelCol='total_amount',\n",
    "    featuresCol='feature_vector',\n",
    "    elasticNetParam=best_alpha,\n",
    "    regParam=best_reg_param\n",
    ")\n",
    "\n",
    "model = lr.fit(vec_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 튜닝된 모델 저장 및 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"/home/ubuntu/working/spark-examples/taxi_pricing_model\"\n",
    "model.save(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 로딩 시에는 사용한 모델 클래스를 따로 불러와서 사용\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "loaded_model = LinearRegression().load(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/05 15:17:29 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
      "java.lang.IllegalStateException: Block broadcast_1490 not found\n",
      "\tat org.apache.spark.storage.BlockInfoManager.$anonfun$unlock$3(BlockInfoManager.scala:293)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.unlock(BlockInfoManager.scala:293)\n",
      "\tat org.apache.spark.storage.BlockManager.releaseLock(BlockManager.scala:1269)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$releaseBlockManagerLock$1(TorrentBroadcast.scala:289)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$releaseBlockManagerLock$1$adapted(TorrentBroadcast.scala:289)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:130)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:142)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:142)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:197)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:142)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:135)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/08/05 15:17:29 ERROR Executor: Exception in task 1.0 in stage 2072.0 (TID 2367): Block broadcast_1490 not found\n",
      "\n",
      "Previous exception in task: Block rdd_6713_1 not found\n",
      "\torg.apache.spark.storage.BlockInfoManager.$anonfun$unlock$3(BlockInfoManager.scala:293)\n",
      "\tscala.Option.getOrElse(Option.scala:189)\n",
      "\torg.apache.spark.storage.BlockInfoManager.unlock(BlockInfoManager.scala:293)\n",
      "\torg.apache.spark.storage.BlockManager.releaseLock(BlockManager.scala:1269)\n",
      "\torg.apache.spark.storage.BlockManager.$anonfun$getLocalValues$4(BlockManager.scala:940)\n",
      "\torg.apache.spark.util.CompletionIterator$$anon$1.completion(CompletionIterator.scala:47)\n",
      "\torg.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:36)\n",
      "\torg.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tscala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)\n",
      "\tscala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\torg.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificColumnarIterator.hasNext(Unknown Source)\n",
      "\torg.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\torg.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\torg.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\torg.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:161)\n",
      "\torg.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:83)\n",
      "\torg.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1(ObjectHashAggregateExec.scala:114)\n",
      "\torg.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$1$adapted(ObjectHashAggregateExec.scala:90)\n",
      "\torg.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:885)\n",
      "\torg.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:885)\n",
      "\torg.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\torg.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\torg.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\torg.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\torg.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\torg.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\torg.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\torg.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\torg.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\torg.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\torg.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\torg.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tjava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tjava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tjava.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/05 15:17:29 ERROR Utils: Uncaught exception in thread Executor task launch worker for task 0.0 in stage 2072.0 (TID 2366)\n",
      "java.lang.NullPointerException\n",
      "\tat org.apache.spark.scheduler.Task.$anonfun$run$2(Task.scala:152)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1471)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:150)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/08/05 15:17:29 ERROR Executor: Exception in task 0.0 in stage 2072.0 (TID 2366): Block rdd_6713_0 not found\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-env-324",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
