{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/05 14:02:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "MAX_MEMORY=\"5g\"\n",
    "spark = SparkSession.builder.appName(\"taxi-fare-prediciton\")\\\n",
    "                .config(\"spark.executor.memory\", MAX_MEMORY)\\\n",
    "                .config(\"spark.driver.memory\", MAX_MEMORY)\\\n",
    "                .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_dir = \"/home/ubuntu/working/spark-examples/data/ml_data_taxi\" # 로컬 디렉토리로 지정. hdfs를 사용 할 수도 있다!\n",
    "\n",
    "train_df = spark.read.parquet(f\"{data_dir}/train/\")\n",
    "test_df  = spark.read.parquet(f\"{data_dir}/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_8de26289538f,\n",
       " OneHotEncoder_a2b8abb24f26,\n",
       " StringIndexer_f63fb8bc8813,\n",
       " OneHotEncoder_0cbde98fb496,\n",
       " StringIndexer_bd07ebed515b,\n",
       " OneHotEncoder_35d56cb78c6c]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stages = []\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "# OneHotEncoding을 수행할 컬럼을 지정\n",
    "cat_features = [\n",
    "    \"pickup_location_id\",\n",
    "    \"dropoff_location_id\",\n",
    "    \"day_of_week\"\n",
    "]\n",
    "\n",
    "for c in cat_features:\n",
    "    # 1. 데이터를 문자열 형식으로 바꿔준다. setHandleInvalid : Null값 같은 데이터를 어떻게 처리 할건지\n",
    "    cat_indexer = StringIndexer(inputCol=c, outputCol=c+\"_idx\").setHandleInvalid(\"keep\")\n",
    "    \n",
    "    # 2. One Hot Encoding 수행\n",
    "    onehot_encoder = OneHotEncoder(inputCols=[cat_indexer.getOutputCol()], outputCols=[c+\"_onehot\"])\n",
    "    \n",
    "    stages += [cat_indexer, onehot_encoder]\n",
    "\n",
    "stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_8de26289538f,\n",
       " OneHotEncoder_a2b8abb24f26,\n",
       " StringIndexer_f63fb8bc8813,\n",
       " OneHotEncoder_0cbde98fb496,\n",
       " StringIndexer_bd07ebed515b,\n",
       " OneHotEncoder_35d56cb78c6c,\n",
       " VectorAssembler_60464bdd3272,\n",
       " StandardScaler_6632593e6ea4,\n",
       " VectorAssembler_b7a09a801998,\n",
       " StandardScaler_3c96c5bd2bce,\n",
       " VectorAssembler_e0010bfe3fa8,\n",
       " StandardScaler_842e7df723c5]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "num_features = [\n",
    "    \"passenger_count\",\n",
    "    \"trip_distance\",\n",
    "    \"pickup_time\"\n",
    "]\n",
    "\n",
    "for n in num_features:\n",
    "    \n",
    "    # 각각의 컬럼의 데이터가 벡터화. ex) 1.5 -> [1.5]\n",
    "    num_assembler = VectorAssembler(inputCols=[n], outputCol=n+\"_vector\")\n",
    "    \n",
    "    # StandardScaling 수행\n",
    "    num_scaler = StandardScaler(inputCol=num_assembler.getOutputCol(), outputCol=n+\"_scaled\")\n",
    "    \n",
    "    stages += [num_assembler, num_scaler]\n",
    "\n",
    "stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_8de26289538f,\n",
       " OneHotEncoder_a2b8abb24f26,\n",
       " StringIndexer_f63fb8bc8813,\n",
       " OneHotEncoder_0cbde98fb496,\n",
       " StringIndexer_bd07ebed515b,\n",
       " OneHotEncoder_35d56cb78c6c,\n",
       " VectorAssembler_60464bdd3272,\n",
       " StandardScaler_6632593e6ea4,\n",
       " VectorAssembler_b7a09a801998,\n",
       " StandardScaler_3c96c5bd2bce,\n",
       " VectorAssembler_e0010bfe3fa8,\n",
       " StandardScaler_842e7df723c5,\n",
       " VectorAssembler_5f623cce4331]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# _onehot이 붙은 컬럼과 _scaled 가 붙은 컬럼만 있으면 된다.\n",
    "assembler_inputs = [c + \"_onehot\" for c in cat_features] + [n + \"_scaled\" for n in num_features]\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"feature_vector\")\n",
    "stages += [assembler]\n",
    "\n",
    "stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 하이퍼 파라미터\n",
    "- 모델이 학습하지 못하는 파라미터\n",
    "- 사람이 직접 모델에 넣어주는 파라미터\n",
    "- 하이퍼 파라미터에 따라서 모델의 성능이 바뀔 수 있다.\n",
    "    - 적절한 최적의 파라미터를 찾아야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(\n",
    "    maxIter=50,\n",
    "    solver='normal',\n",
    "    labelCol=\"total_amount\",\n",
    "    featuresCol='feature_vector'\n",
    ")\n",
    "\n",
    "cv_stages = stages + [lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_8de26289538f,\n",
       " OneHotEncoder_a2b8abb24f26,\n",
       " StringIndexer_f63fb8bc8813,\n",
       " OneHotEncoder_0cbde98fb496,\n",
       " StringIndexer_bd07ebed515b,\n",
       " OneHotEncoder_35d56cb78c6c,\n",
       " VectorAssembler_60464bdd3272,\n",
       " StandardScaler_6632593e6ea4,\n",
       " VectorAssembler_b7a09a801998,\n",
       " StandardScaler_3c96c5bd2bce,\n",
       " VectorAssembler_e0010bfe3fa8,\n",
       " StandardScaler_842e7df723c5,\n",
       " VectorAssembler_5f623cce4331,\n",
       " LinearRegression_ad26a366029e]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline_79f2195c0218"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_pipeline = Pipeline(stages=cv_stages)\n",
    "cv_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch 및 CrossValidation 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossValidator_d78ac463937a"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "param_grid = ParamGridBuilder()\\\n",
    "                .addGrid(lr.elasticNetParam, [0.1, 0.2, 0.3, 0.4, 0.5])\\\n",
    "                .addGrid(lr.regParam, [0.01, 0.02, 0.03, 0.04, 0.05])\\\n",
    "                .build()\n",
    "\n",
    "cross_val = CrossValidator(\n",
    "    estimator=cv_pipeline, # 파이프라인을 estimator로 사용하는 경우 제일 마지막 스테이지가 모델이어야 한다.\n",
    "    estimatorParamMaps=param_grid, # 설정하지 않으면 GridSearch 없이 Cross Validation만 진행\n",
    "    evaluator=RegressionEvaluator(labelCol=\"total_amount\"),\n",
    "    numFolds=5\n",
    ")\n",
    "\n",
    "cross_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- pickup_location_id: integer (nullable = true)\n",
      " |-- dropoff_location_id: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- pickup_time: integer (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 임의의 샘플 데이터 세트 만들기. 전체로 다 하면 시간이 많이 걸려서....\n",
    "toy_df = train_df.sample(False, 0.1, seed=1)\n",
    "toy_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/05 14:03:46 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/08/05 14:03:46 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "23/08/05 14:03:49 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "23/08/05 14:03:49 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "[Stage 230:>                                                        (0 + 2) / 2]\r"
     ]
    }
   ],
   "source": [
    "cv_model = cross_val.fit(toy_df)\n",
    "cv_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BestModel 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = cv_model.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Parameter 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha = cv_model.bestModel.stages[-1]._java_obj.getElasticNetParam()\n",
    "best_reg_param = cv_model.bestModel.stages[-1]._java_obj.getRegParam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전체 데이터를 대상으로 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=stages) # 모델이 빠진 순수하게 전처리만 하는 파이프라인 생성\n",
    "fitted_transformer = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_train_df = fitted_transformer.transform(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best paramter로 모델 생성\n",
    "lr = LinearRegression(\n",
    "    maxIter=50,\n",
    "    solver='normal',\n",
    "    labelCol='total_amount',\n",
    "    featuresCol='feature_vector',\n",
    "    elasticNetParam=best_alpha,\n",
    "    regParam=best_reg_param\n",
    ")\n",
    "\n",
    "model = lr.fit(vec_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 튜닝된 모델 저장 및 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"/home/ubuntu/working/spark-examples/taxi_pricing_model\"\n",
    "model.save(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 로딩 시에는 사용한 모델 클래스를 따로 불러와서 사용\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "loaded_model = LinearRegression().load(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-env-324",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
