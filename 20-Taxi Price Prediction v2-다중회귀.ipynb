{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/05 11:48:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"trip_count_sql\").getOrCreate()\n",
    "\n",
    "directory=\"/home/ubuntu/working/spark-examples/data/\"\n",
    "trip_files=\"trips/*\"\n",
    "\n",
    "trips_df = spark.read.csv(f\"file://{directory}/{trip_files}\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trips_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_df.createOrReplaceTempView(\"trips\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "달라지는 점\n",
    "- v1에서 만든 모델은 데이터의 종류가 너무 적어서 예측 성능이 좋지 못해 과소적합이 발생\n",
    "- 데이터의 종류를 늘리는 다항 회귀를 활용\n",
    "    - $\\hat{y} =w1x1+w1x2 +...+b =WX + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    passenger_count,\n",
    "    PULocationID as pickup_location_id,\n",
    "    DOLocationID as dropoff_location_id,\n",
    "    trip_distance,\n",
    "    HOUR(tpep_pickup_datetime) as pickup_time,\n",
    "    DATE_FORMAT(TO_DATE(tpep_pickup_datetime), 'EEEE') AS day_of_week,\n",
    "    total_amount\n",
    "FROM\n",
    "    trips\n",
    "WHERE\n",
    "    total_amount < 5000\n",
    "    AND total_amount > 0\n",
    "    AND trip_distance > 0\n",
    "    AND trip_distance < 500\n",
    "    AND passenger_count < 5\n",
    "    AND TO_DATE(tpep_pickup_datetime) >= '2021-01-01'\n",
    "    AND TO_DATE(tpep_pickup_datetime) < '2021-08-01'\n",
    "\"\"\"\n",
    "data_df = spark.sql(query)\n",
    "data_df.createOrReplaceTempView(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+-------------------+-------------+-----------+-----------+------------+\n",
      "|passenger_count|pickup_location_id|dropoff_location_id|trip_distance|pickup_time|day_of_week|total_amount|\n",
      "+---------------+------------------+-------------------+-------------+-----------+-----------+------------+\n",
      "|              0|               138|                265|         16.5|          0|     Monday|       70.07|\n",
      "|              1|                68|                264|         1.13|          0|     Monday|       11.16|\n",
      "|              1|               239|                262|         2.68|          0|     Monday|       18.59|\n",
      "|              1|               186|                 91|         12.4|          0|     Monday|        43.8|\n",
      "|              2|               132|                265|          9.7|          0|     Monday|        32.3|\n",
      "|              1|               138|                141|          9.3|          0|     Monday|       43.67|\n",
      "|              1|               138|                 50|         9.58|          0|     Monday|        46.1|\n",
      "|              1|               132|                123|         16.2|          0|     Monday|        45.3|\n",
      "|              1|               140|                  7|         3.58|          0|     Monday|        19.3|\n",
      "|              1|               239|                238|         0.91|          0|     Monday|        14.8|\n",
      "|              2|               116|                 41|         2.57|          0|     Monday|        12.8|\n",
      "|              1|                74|                 41|          0.4|          0|     Monday|         5.3|\n",
      "|              1|               239|                144|         3.26|          0|     Monday|        17.3|\n",
      "|              1|               132|                 91|        13.41|          0|     Monday|       47.25|\n",
      "|              2|               132|                230|         18.3|          0|     Monday|       61.42|\n",
      "|              1|               229|                 48|         1.53|          0|     Monday|       14.16|\n",
      "|              1|                48|                 68|          2.0|          0|     Monday|        11.8|\n",
      "|              2|               132|                255|         16.6|          0|     Monday|       54.96|\n",
      "|              1|               132|                145|         15.5|          0|     Monday|       56.25|\n",
      "|              2|                79|                164|          1.3|          0|     Monday|        16.8|\n",
      "+---------------+------------------+-------------------+-------------+-----------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = data_df.randomSplit([0.8,0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금처럼 데이터의 양이 너무나 많고, 그 데이터에 대한 전처리를 수행 했음.\n",
    "- 시간이 매우 많이 걸리는 배치 작업\n",
    "- 추후에 다시 이 데이터를 활용한다면 다시 처음부터 전처리 하는데 시간이 많이 걸린다.\n",
    "- 어떻게 전처리가 된 데이터를 파일이나 데이터 베이스에 저장 해놓고, 나중에 다시 불러오는 것이 시간적으로 이득"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path file:/home/ubuntu/working/spark-examples/data/ml_data_taxi/train already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m data_dir \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/home/ubuntu/working/spark-examples/data/ml_data_taxi\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m# 로컬 디렉토리로 지정. hdfs를 사용 할 수도 있다!\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# hdfs_dir = \"hdfs://user/ubuntu/spark-taxi-data\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[39m# Spark Dataframe의 write를 이용해서 데이터를 파일 또는 DB에 저장이 가능\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m train_df\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39;49mformat(\u001b[39m\"\u001b[39;49m\u001b[39mparquet\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49msave(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mdata_dir\u001b[39m}\u001b[39;49;00m\u001b[39m/train/\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      7\u001b[0m test_df\u001b[39m.\u001b[39mwrite\u001b[39m.\u001b[39mformat(\u001b[39m\"\u001b[39m\u001b[39mparquet\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39msave(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mdata_dir\u001b[39m}\u001b[39;00m\u001b[39m/test/\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/spark-env-324/lib/python3.8/site-packages/pyspark/sql/readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jwrite\u001b[39m.\u001b[39msave()\n\u001b[1;32m    739\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 740\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49msave(path)\n",
      "File \u001b[0;32m~/miniconda3/envs/spark-env-324/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/miniconda3/envs/spark-env-324/lib/python3.8/site-packages/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: path file:/home/ubuntu/working/spark-examples/data/ml_data_taxi/train already exists."
     ]
    }
   ],
   "source": [
    "# 파케이(parquet) 형식으로 데이터 마트를 저장\n",
    "data_dir = \"/home/ubuntu/working/spark-examples/data/ml_data_taxi\" # 로컬 디렉토리로 지정. hdfs를 사용 할 수도 있다!\n",
    "# hdfs_dir = \"hdfs://user/ubuntu/spark-taxi-data\"\n",
    "\n",
    "# Spark Dataframe의 write를 이용해서 데이터를 파일 또는 DB에 저장이 가능\n",
    "train_df.write.format(\"parquet\").save(f\"{data_dir}/train/\")\n",
    "test_df.write.format(\"parquet\").save(f\"{data_dir}/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 분산 저장된 데이터를 불러오기\n",
    "# 불러올 때 파일을 지정하는게 아니고 디렉토리를 지정하면 된다\n",
    "train_df = spark.read.parquet(f\"{data_dir}/train/\")\n",
    "test_df = spark.read.parquet(f\"{data_dir}/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- pickup_location_id: integer (nullable = true)\n",
      " |-- dropoff_location_id: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- pickup_time: integer (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파이프라인 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline에 넣을 과정(stage)를 하나씩 넣어 놓을 리스트 선언\n",
    "stages = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. OneHotEncoding Stage\n",
    "- `pickup_location_id`\n",
    "- `ropoff_location_id`\n",
    "- `day_of_week`\n",
    "\n",
    "`pickup_location_id`, `dropoff_location_id`는 숫자 형식의 데이터!\n",
    "- 숫자 형식의 데이터는 OneHotEncoding이 되지 않는다.\n",
    "- 따라서 숫자 형식의 카테고리 데이터를 임시로 문자열로 처리하기 위히 `StringIndexer` 트랜스포머 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_bf3e79a17389,\n",
       " OneHotEncoder_220d38b6d68d,\n",
       " StringIndexer_e9177f25e5d1,\n",
       " OneHotEncoder_7c0b44191e81,\n",
       " StringIndexer_01ea8c95220e,\n",
       " OneHotEncoder_755da490c292]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "# OneHotEncoding을 수행할 컬럼을 지정\n",
    "\n",
    "cat_features = [\n",
    "    \"pickup_location_id\",\n",
    "    \"dropoff_location_id\",\n",
    "    \"day_of_week\"\n",
    "]\n",
    "\n",
    "for c in cat_features:\n",
    "    # 1. 데이터를 문자열 형식으로 바꿔준다.\n",
    "    # setHandleInvalid : Null 값 같은 데이터를 어떻게 처리 할지.\n",
    "    cat_indexer = StringIndexer(inputCol=c, outputCol=c+\"_idx\").setHandleInvalid(\"keep\")\n",
    "    # 2. OneHotEncoding\n",
    "    onehot_encoder = OneHotEncoder(inputCols=[cat_indexer.getOutputCol()], outputCols=[c+\"_onehot\"]) # 여러개의 입력을 받을 수 있지만(cols) 이번에는 하나씩 처리\n",
    "    stages +=[cat_indexer, onehot_encoder]\n",
    "stages # 뒤에 붙는 값은 메모리 주소값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. StandardScaler & VectorAssembler Stage\n",
    "- `passenger_count`\n",
    "- `trip_distance`\n",
    "- `pickup_time`\n",
    "\n",
    "기본적으로 스케일링 작업은 스칼라 값이 아닌, 벡터 단위로 스케일링이 일어나야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_bf3e79a17389,\n",
       " OneHotEncoder_220d38b6d68d,\n",
       " StringIndexer_e9177f25e5d1,\n",
       " OneHotEncoder_7c0b44191e81,\n",
       " StringIndexer_01ea8c95220e,\n",
       " OneHotEncoder_755da490c292,\n",
       " VectorAssembler_f33bd9586e9c,\n",
       " StandardScaler_fa0dca526b0b,\n",
       " VectorAssembler_87f17f77ba06,\n",
       " StandardScaler_2f7309204905,\n",
       " VectorAssembler_368849acf12d,\n",
       " StandardScaler_6d208c70a452]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "num_features = [\n",
    "    \"passenger_count\",\n",
    "    \"trip_distance\", \n",
    "    \"pickup_time\"\n",
    "]\n",
    "\n",
    "for n in num_features:\n",
    "    # 1. 각각의 컬럼의 데이터를 벡터화. ex) 1.5 -> [1.5]\n",
    "    num_assembler = VectorAssembler(inputCols=[n], outputCol=n+\"_vector\")\n",
    "\n",
    "    # 2. StandardScaling 수행\n",
    "    num_scaler = StandardScaler(inputCol=num_assembler.getOutputCol(), outputCol=n+\"_scaled\")\n",
    "\n",
    "    stages += [num_assembler, num_scaler]\n",
    "\n",
    "stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "머신러닝을 위한 Preprocessing된 결과물 벡터를 하나로 합쳐야 훈련 가능한 데이터가 됩니다. `VectorAssembler`를 사용해서 합친다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pickup_location_id_onehot',\n",
       " 'dropoff_location_id_onehot',\n",
       " 'day_of_week_onehot',\n",
       " 'passenger_count_scaled',\n",
       " 'trip_distance_scaled',\n",
       " 'pickup_time_scaled']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assemble 할 데이터는? OneHotEncoding이 되어 있거나, Scaled된 데이터를 Assemble\n",
    "assemble_inputs = [c+\"_onehot\" for c in cat_features] + [n + \"_scaled\" for n in num_features]\n",
    "assemble_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_bf3e79a17389,\n",
       " OneHotEncoder_220d38b6d68d,\n",
       " StringIndexer_e9177f25e5d1,\n",
       " OneHotEncoder_7c0b44191e81,\n",
       " StringIndexer_01ea8c95220e,\n",
       " OneHotEncoder_755da490c292,\n",
       " VectorAssembler_f33bd9586e9c,\n",
       " StandardScaler_fa0dca526b0b,\n",
       " VectorAssembler_87f17f77ba06,\n",
       " StandardScaler_2f7309204905,\n",
       " VectorAssembler_368849acf12d,\n",
       " StandardScaler_6d208c70a452,\n",
       " VectorAssembler_3fe0572d4caf]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_assembler = VectorAssembler(inputCols=assemble_inputs, outputCol=\"features\")\n",
    "stages.append(total_assembler)\n",
    "stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline_79151741a037"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 파이프라인 등록\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=stages)\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PipelineModel_1dbdc60d3729"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit\n",
    "# 어떻게 할 건지 계획이 세워진 단계\n",
    "fitted_transformer = pipeline.fit(train_df)\n",
    "fitted_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- pickup_location_id: integer (nullable = true)\n",
      " |-- dropoff_location_id: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- pickup_time: integer (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- pickup_location_id_idx: double (nullable = false)\n",
      " |-- pickup_location_id_onehot: vector (nullable = true)\n",
      " |-- dropoff_location_id_idx: double (nullable = false)\n",
      " |-- dropoff_location_id_onehot: vector (nullable = true)\n",
      " |-- day_of_week_idx: double (nullable = false)\n",
      " |-- day_of_week_onehot: vector (nullable = true)\n",
      " |-- passenger_count_vector: vector (nullable = true)\n",
      " |-- passenger_count_scaled: vector (nullable = true)\n",
      " |-- trip_distance_vector: vector (nullable = true)\n",
      " |-- trip_distance_scaled: vector (nullable = true)\n",
      " |-- pickup_time_vector: vector (nullable = true)\n",
      " |-- pickup_time_scaled: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec_train_df = fitted_transformer.transform(train_df)\n",
    "vec_train_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+-------------------+-------------+-----------+-----------+------------+----------------------+-------------------------+-----------------------+--------------------------+---------------+------------------+----------------------+----------------------+--------------------+--------------------+------------------+--------------------+--------------------+\n",
      "|passenger_count|pickup_location_id|dropoff_location_id|trip_distance|pickup_time|day_of_week|total_amount|pickup_location_id_idx|pickup_location_id_onehot|dropoff_location_id_idx|dropoff_location_id_onehot|day_of_week_idx|day_of_week_onehot|passenger_count_vector|passenger_count_scaled|trip_distance_vector|trip_distance_scaled|pickup_time_vector|  pickup_time_scaled|            features|\n",
      "+---------------+------------------+-------------------+-------------+-----------+-----------+------------+----------------------+-------------------------+-----------------------+--------------------------+---------------+------------------+----------------------+----------------------+--------------------+--------------------+------------------+--------------------+--------------------+\n",
      "|              0|                 4|                  4|          0.1|          2|     Sunday|         6.8|                  62.0|         (263,[62],[1.0])|                   49.0|          (261,[49],[1.0])|            6.0|     (7,[6],[1.0])|                 [0.0]|                 [0.0]|               [0.1]|[0.02606818275092...|             [2.0]|[0.3905897140442335]|(534,[62,312,530,...|\n",
      "|              0|                 4|                  4|          2.2|          2|   Saturday|        15.3|                  62.0|         (263,[62],[1.0])|                   49.0|          (261,[49],[1.0])|            4.0|     (7,[4],[1.0])|                 [0.0]|                 [0.0]|               [2.2]|[0.5735000205202505]|             [2.0]|[0.3905897140442335]|(534,[62,312,528,...|\n",
      "|              0|                 4|                 48|          2.8|         16|   Saturday|        19.3|                  62.0|         (263,[62],[1.0])|                   10.0|          (261,[10],[1.0])|            4.0|     (7,[4],[1.0])|                 [0.0]|                 [0.0]|               [2.8]|[0.7299091170257733]|            [16.0]| [3.124717712353868]|(534,[62,273,528,...|\n",
      "|              0|                 4|                 79|          0.6|         14|   Thursday|         8.3|                  62.0|         (263,[62],[1.0])|                   18.0|          (261,[18],[1.0])|            1.0|     (7,[1],[1.0])|                 [0.0]|                 [0.0]|               [0.6]|[0.15640909650552...|            [14.0]|[2.7341279983096345]|(534,[62,281,525,...|\n",
      "|              0|                 4|                 87|          2.7|         15|     Friday|        15.8|                  62.0|         (263,[62],[1.0])|                   46.0|          (261,[46],[1.0])|            0.0|     (7,[0],[1.0])|                 [0.0]|                 [0.0]|               [2.7]| [0.703840934274853]|            [15.0]| [2.929422855331751]|(534,[62,309,524,...|\n",
      "+---------------+------------------+-------------------+-------------+-----------+-----------+------------+----------------------+-------------------------+-----------------------+--------------------------+---------------+------------------+----------------------+----------------------+--------------------+--------------------+------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec_train_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(534,[62,312,530,...|\n",
      "|(534,[62,312,528,...|\n",
      "|(534,[62,273,528,...|\n",
      "|(534,[62,281,525,...|\n",
      "|(534,[62,309,524,...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec_train_df.select(\"features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression_d2654d96d49d"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(\n",
    "    maxIter=50,\n",
    "    solver='normal',\n",
    "    labelCol='total_amount',\n",
    "    featuresCol='features',\n",
    "    regParam=0.1\n",
    ")\n",
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/05 12:03:24 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/08/05 12:03:24 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "23/08/05 12:03:31 WARN InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegressionModel: uid=LinearRegression_d2654d96d49d, numFeatures=534"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lr.fit(vec_train_df)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 테스트 데이터 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- pickup_location_id: integer (nullable = true)\n",
      " |-- dropoff_location_id: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- pickup_time: integer (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- pickup_location_id_idx: double (nullable = false)\n",
      " |-- pickup_location_id_onehot: vector (nullable = true)\n",
      " |-- dropoff_location_id_idx: double (nullable = false)\n",
      " |-- dropoff_location_id_onehot: vector (nullable = true)\n",
      " |-- day_of_week_idx: double (nullable = false)\n",
      " |-- day_of_week_onehot: vector (nullable = true)\n",
      " |-- passenger_count_vector: vector (nullable = true)\n",
      " |-- passenger_count_scaled: vector (nullable = true)\n",
      " |-- trip_distance_vector: vector (nullable = true)\n",
      " |-- trip_distance_scaled: vector (nullable = true)\n",
      " |-- pickup_time_vector: vector (nullable = true)\n",
      " |-- pickup_time_scaled: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test_df 변환\n",
    "vec_test_df = fitted_transformer.transform(test_df)\n",
    "vec_test_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+------------------+\n",
      "|            features|total_amount|        prediction|\n",
      "+--------------------+------------+------------------+\n",
      "|(534,[62,280,525,...|       12.25| 13.29492360925461|\n",
      "|(534,[62,267,528,...|        14.3|15.905557244889836|\n",
      "|(534,[63,320,529,...|         7.3| 10.39572555474729|\n",
      "|(534,[63,281,524,...|        25.3|22.111946306699195|\n",
      "|(534,[63,334,529,...|        11.3|11.735752886082826|\n",
      "|(534,[63,311,525,...|        17.8|28.378860790583005|\n",
      "|(534,[63,371,525,...|         5.8| 7.550991936520331|\n",
      "|(534,[232,482,525...|        65.3| 45.51727780460939|\n",
      "|(534,[84,300,525,...|       63.85| 68.36065981929308|\n",
      "|(534,[84,295,525,...|        63.3| 76.75903650222011|\n",
      "|(534,[84,290,528,...|       61.85|62.508610383271844|\n",
      "|(534,[84,287,526,...|        76.3| 67.92073246313662|\n",
      "|(534,[71,312,524,...|        15.8| 19.73848400629462|\n",
      "|(534,[71,281,529,...|        16.8|20.742519464873958|\n",
      "|(534,[71,293,529,...|        24.3| 26.82600119038327|\n",
      "|(534,[71,290,529,...|       23.15| 23.61591962106631|\n",
      "|(534,[47,307,529,...|         6.8|13.054123561928407|\n",
      "|(534,[47,307,524,...|        9.45|14.288796392352175|\n",
      "|(534,[47,277,527,...|        15.3|18.051785806646564|\n",
      "|(534,[47,277,530,...|        17.8|19.352882428554242|\n",
      "+--------------------+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# vec_test_df로 예측\n",
    "predictions = model.transform(vec_test_df)\n",
    "predictions.select(\"features\", \"total_amount\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[passenger_count: int, pickup_location_id: int, dropoff_location_id: int, trip_distance: double, pickup_time: int, day_of_week: string, total_amount: double, pickup_location_id_idx: double, pickup_location_id_onehot: vector, dropoff_location_id_idx: double, dropoff_location_id_onehot: vector, day_of_week_idx: double, day_of_week_onehot: vector, passenger_count_vector: vector, passenger_count_scaled: vector, trip_distance_vector: vector, trip_distance_scaled: vector, pickup_time_vector: vector, pickup_time_scaled: vector, features: vector, prediction: double]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predictions 데이터를 이용하기 전에는 캐싱을 해주는 것이 좋다.\n",
    "# 예측한 결과는 거의 항상 조회만 일어나기 때문...\n",
    "predictions.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.893589079571408"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7957351622192416"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-env-324",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
