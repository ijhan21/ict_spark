{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/29 14:04:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-30-105.ap-northeast-2.compute.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>trip_count_sql</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f9d3578ac70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"trip_count_sql\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/working/spark-examples/data/fhvhv_tripdata_2020-03.csv'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "trip_file = 'fhvhv_tripdata_2020-03.csv'\n",
    "directory = '/home/ubuntu/working/spark-examples/data/'\n",
    "zone_file = 'taxi+_zone_lookup.csv'\n",
    "trip_file_path = os.path.join(directory, trip_file)\n",
    "zone_file_path = os.path.join(directory, zone_file)\n",
    "trip_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0005|              B02510|2020-03-01 00:03:40|2020-03-01 00:23:39|          81|         159|   null|\n",
      "|           HV0005|              B02510|2020-03-01 00:28:05|2020-03-01 00:38:57|         168|         119|   null|\n",
      "|           HV0003|              B02764|2020-03-01 00:03:07|2020-03-01 00:15:04|         137|         209|      1|\n",
      "|           HV0003|              B02764|2020-03-01 00:18:42|2020-03-01 00:38:42|         209|          80|   null|\n",
      "|           HV0003|              B02764|2020-03-01 00:44:24|2020-03-01 00:58:44|         256|         226|   null|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trip_data = spark.read.csv(f\"file:///{trip_file_path}\", inferSchema=True, header=True)\n",
    "trip_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zone_data = spark.read.csv(f\"{zone_file_path}\", inferSchema=True, header=True)\n",
    "zone_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"SELECT zone_data.Zone, count(*) AS trips\n",
    "FROM trip_data JOIN zone_data ON trip_data.PULocationID = zone_data.LocationID\n",
    "WHERE trip_data.hvfhs_license_num = 'HV0003'\n",
    "GROUP BY zone_data.Zone \n",
    "order by trips desc\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 임시뷰를 만든다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_data.createOrReplaceTempView(\"zone_data\")\n",
    "trip_data.createOrReplaceTempView(\"trip_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0005|              B02510|2020-03-01 00:03:40|2020-03-01 00:23:39|          81|         159|   null|\n",
      "|           HV0005|              B02510|2020-03-01 00:28:05|2020-03-01 00:38:57|         168|         119|   null|\n",
      "|           HV0003|              B02764|2020-03-01 00:03:07|2020-03-01 00:15:04|         137|         209|      1|\n",
      "|           HV0003|              B02764|2020-03-01 00:18:42|2020-03-01 00:38:42|         209|          80|   null|\n",
      "|           HV0003|              B02764|2020-03-01 00:44:24|2020-03-01 00:58:44|         256|         226|   null|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    select * from trip_data\n",
    "    limit 5;\n",
    "\"\"\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    select * from zone_data\n",
    "    limit 5;\n",
    "\"\"\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|                Zone| trips|\n",
      "+--------------------+------+\n",
      "| Crown Heights North|163091|\n",
      "|       East New York|134198|\n",
      "|         JFK Airport|114179|\n",
      "|        East Village|112017|\n",
      "|      Bushwick South|110150|\n",
      "|Central Harlem North|108070|\n",
      "|   LaGuardia Airport|104119|\n",
      "|Washington Height...| 97324|\n",
      "|Flatbush/Ditmas Park| 95724|\n",
      "|            Canarsie| 94484|\n",
      "|TriBeCa/Civic Center| 94155|\n",
      "|             Astoria| 92676|\n",
      "|             Bedford| 90352|\n",
      "|      Midtown Center| 90261|\n",
      "|  Stuyvesant Heights| 88749|\n",
      "|            Union Sq| 88372|\n",
      "|Times Sq/Theatre ...| 86870|\n",
      "|Prospect-Lefferts...| 84347|\n",
      "|         Brownsville| 82764|\n",
      "|Mott Haven/Port M...| 82396|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Sort ['trips DESC NULLS LAST], true\n",
      "+- 'Aggregate ['zone_data.Zone], ['zone_data.Zone, 'count(1) AS trips#598]\n",
      "   +- 'Filter ('trip_data.hvfhs_license_num = HV0003)\n",
      "      +- 'Join Inner, ('trip_data.PULocationID = 'zone_data.LocationID)\n",
      "         :- 'UnresolvedRelation [trip_data], [], false\n",
      "         +- 'UnresolvedRelation [zone_data], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "Zone: string, trips: bigint\n",
      "Sort [trips#598L DESC NULLS LAST], true\n",
      "+- Aggregate [Zone#483], [Zone#483, count(1) AS trips#598L]\n",
      "   +- Filter (hvfhs_license_num#413 = HV0003)\n",
      "      +- Join Inner, (PULocationID#417 = LocationID#481)\n",
      "         :- SubqueryAlias trip_data\n",
      "         :  +- View (`trip_data`, [hvfhs_license_num#413,dispatching_base_num#414,pickup_datetime#415,dropoff_datetime#416,PULocationID#417,DOLocationID#418,SR_Flag#419])\n",
      "         :     +- Relation [hvfhs_license_num#413,dispatching_base_num#414,pickup_datetime#415,dropoff_datetime#416,PULocationID#417,DOLocationID#418,SR_Flag#419] csv\n",
      "         +- SubqueryAlias zone_data\n",
      "            +- View (`zone_data`, [LocationID#481,Borough#482,Zone#483,service_zone#484])\n",
      "               +- Relation [LocationID#481,Borough#482,Zone#483,service_zone#484] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [trips#598L DESC NULLS LAST], true\n",
      "+- Aggregate [Zone#483], [Zone#483, count(1) AS trips#598L]\n",
      "   +- Project [Zone#483]\n",
      "      +- Join Inner, (PULocationID#417 = LocationID#481)\n",
      "         :- Project [PULocationID#417]\n",
      "         :  +- Filter ((isnotnull(hvfhs_license_num#413) AND (hvfhs_license_num#413 = HV0003)) AND isnotnull(PULocationID#417))\n",
      "         :     +- Relation [hvfhs_license_num#413,dispatching_base_num#414,pickup_datetime#415,dropoff_datetime#416,PULocationID#417,DOLocationID#418,SR_Flag#419] csv\n",
      "         +- Project [LocationID#481, Zone#483]\n",
      "            +- Filter isnotnull(LocationID#481)\n",
      "               +- Relation [LocationID#481,Borough#482,Zone#483,service_zone#484] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [trips#598L DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(trips#598L DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=417]\n",
      "      +- HashAggregate(keys=[Zone#483], functions=[count(1)], output=[Zone#483, trips#598L])\n",
      "         +- Exchange hashpartitioning(Zone#483, 200), ENSURE_REQUIREMENTS, [plan_id=414]\n",
      "            +- HashAggregate(keys=[Zone#483], functions=[partial_count(1)], output=[Zone#483, count#603L])\n",
      "               +- Project [Zone#483]\n",
      "                  +- BroadcastHashJoin [PULocationID#417], [LocationID#481], Inner, BuildRight, false\n",
      "                     :- Project [PULocationID#417]\n",
      "                     :  +- Filter ((isnotnull(hvfhs_license_num#413) AND (hvfhs_license_num#413 = HV0003)) AND isnotnull(PULocationID#417))\n",
      "                     :     +- FileScan csv [hvfhs_license_num#413,PULocationID#417] Batched: false, DataFilters: [isnotnull(hvfhs_license_num#413), (hvfhs_license_num#413 = HV0003), isnotnull(PULocationID#417)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/ubuntu/working/spark-examples/data/fhvhv_tripdata_2020-03.csv], PartitionFilters: [], PushedFilters: [IsNotNull(hvfhs_license_num), EqualTo(hvfhs_license_num,HV0003), IsNotNull(PULocationID)], ReadSchema: struct<hvfhs_license_num:string,PULocationID:int>\n",
      "                     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=409]\n",
      "                        +- Filter isnotnull(LocationID#481)\n",
      "                           +- FileScan csv [LocationID#481,Zone#483] Batched: false, DataFilters: [isnotnull(LocationID#481)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/ubuntu/working/spark-examples/data/taxi+_zone_lookup.csv], PartitionFilters: [], PushedFilters: [IsNotNull(LocationID)], ReadSchema: struct<LocationID:int,Zone:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query).explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
