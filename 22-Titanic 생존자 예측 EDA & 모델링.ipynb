{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"titanic\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"/home/ubuntu/working/spark-examples/data/titanic_train.csv\"\n",
    "titanic_sdf = spark.read.csv(filepath, inferSchema=True, header=True)\n",
    "\n",
    "titanic_sdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_sdf.createOrReplaceTempView(\"titanic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 생존자 수 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT survived, count(*) as cnt\n",
    "FROM titanic\n",
    "GROUP BY survived\n",
    "\"\"\"\n",
    "\n",
    "sns.barplot(x='survived', y='cnt', data=spark.sql(query).toPandas())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pclass 별 인원 파악하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT Pclass, count(*) as cnt\n",
    "FROM titanic\n",
    "GROUP BY Pclass\n",
    "\"\"\"\n",
    "\n",
    "sns.barplot(x='Pclass', y='cnt', data=spark.sql(query).toPandas())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pclass 별 생존/사망자 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT Pclass, survived, count(*) as cnt\n",
    "FROM titanic\n",
    "GROUP BY Pclass, survived\n",
    "\"\"\"\n",
    "\n",
    "sns.barplot(x='Pclass', y='cnt', hue='survived', data=spark.sql(query).toPandas())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 각 Cabin 별 탑승객 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT Cabin, count(*) as cnt\n",
    "FROM titanic\n",
    "GROUP BY Cabin\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "뒤의 숫자 때문에 정확히 각 구역 별 몇 명인지 알 수가 없을 것 같다. 앞에 알파벳만 가져오자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT substr(Cabin, 0, 1) as section\n",
    "FROM titanic\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "null 값이 너무 많기 때문에 가장 많이 등장한 `section`으로 `null`을 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT substr(Cabin, 0, 1) as section, survived, count(*) as cnt\n",
    "FROM titanic\n",
    "GROUP BY section, survived\n",
    "ORDER BY section\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C 구역의 인원이 가장 많기 때문에 Cabin 정보가 없는 사람은 임의로 `C` 구역으로 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "titanic_sdf = titanic_sdf.withColumn(\"Section\", F.substring(F.col(\"Cabin\"), 0, 1))\n",
    "titanic_sdf = titanic_sdf.fillna(value=\"C\", subset=[\"Section\"])\n",
    "titanic_sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 Section 별로 몇 명이 살았는지 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT substr(Cabin, 0, 1) as section, survived, count(*) as cnt\n",
    "FROM titanic\n",
    "GROUP BY section, survived\n",
    "ORDER BY section\n",
    "\"\"\"\n",
    "\n",
    "sns.barplot(x='section', y='cnt', hue='survived', data=spark.sql(query).toPandas())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 나이대 별 탑승자 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# udf 정의\n",
    "def get_age_category(age):\n",
    "    cat = ''\n",
    "    \n",
    "    if age <= 5: cat = 'Baby'\n",
    "    elif age <= 12: cat = 'Child'\n",
    "    elif age <= 18: cat = 'Teenager'\n",
    "    elif age <= 25: cat = 'Student'\n",
    "    elif age <= 35: cat = 'Young Adult'\n",
    "    elif age <= 60: cat = 'Adult'\n",
    "    else : cat = 'Elderly'\n",
    "    \n",
    "    return cat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.udf.register(\"get_age_category\", get_age_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age null 처리\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "avg_age = titanic_sdf.select(F.avg(F.col(\"Age\")))\n",
    "avg_age_row = avg_age.first()\n",
    "avg_age_value = avg_age_row[0]\n",
    "\n",
    "titanic_sdf = titanic_sdf.fillna(value=avg_age_value, subset=[\"Age\"])\n",
    "titanic_sdf.createOrReplaceTempView(\"titanic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT get_age_category(Age) as age_cat, count(*) as cnt\n",
    "FROM titanic\n",
    "GROUP BY age_cat\n",
    "\"\"\"\n",
    "\n",
    "sns.barplot(x='age_cat', y='cnt', data=spark.sql(query).toPandas())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 나이대 별 생존자 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT get_age_category(Age) as age_cat, survived, count(*) as cnt\n",
    "FROM titanic\n",
    "GROUP BY age_cat, survived\n",
    "\"\"\"\n",
    "plt.figure(figsize=(20, 8))\n",
    "sns.barplot(x='age_cat', y='cnt', hue='survived', data=spark.sql(query).toPandas())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 동승자 수에 따른 생존자 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-1. Parch, Sibsp 따로 분석하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parch 별 생존자\n",
    "parch_query = \"\"\"\n",
    "SELECT Parch, survived, count(*) as cnt\n",
    "FROM titanic\n",
    "GROUP BY Parch, survived\n",
    "\"\"\"\n",
    "\n",
    "# Sibsp 별 생존자\n",
    "sibsp_query = \"\"\"\n",
    "SELECT Sibsp, survived, count(*) as cnt\n",
    "FROM titanic\n",
    "GROUP BY Sibsp, survived\n",
    "\"\"\"\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "plt.subplot(121)\n",
    "sns.barplot(x='Parch', y='cnt', hue='survived', data=spark.sql(parch_query).toPandas())\n",
    "\n",
    "plt.subplot(122)\n",
    "sns.barplot(x='Sibsp', y='cnt', hue='survived', data=spark.sql(sibsp_query).toPandas())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-2 `Parch` + `Sibsp`를 합쳐 `FamilySize` 구해서 확인하기\n",
    "가족 구성원 자체가 많이 없을 수록 사망자가 압도적으로 높기 때문에 두 데이터를 합쳐서 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT (Parch + Sibsp) as FamilySize, survived, count(*) as cnt\n",
    "FROM titanic\n",
    "GROUP BY FamilySize, survived\n",
    "ORDER BY FamilySize\n",
    "\"\"\"\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "sns.barplot(x='FamilySize', y='cnt', hue='survived', data=spark.sql(query).toPandas())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.성별 탑승자 및 사망/생존 여부 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 탑승자 확인.\n",
    "query = \"\"\"\n",
    "SELECT Sex, count(*) as cnt\n",
    "FROM titanic\n",
    "GROUP BY Sex\n",
    "\"\"\"\n",
    "\n",
    "sns.barplot(x='Sex', y='cnt', data=spark.sql(query).toPandas())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "남성이 더 많이 탑승했다는 것을 확인. 생존 및 사망자도 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 탑승자 확인.\n",
    "query = \"\"\"\n",
    "SELECT Sex, Survived, count(*) as cnt\n",
    "FROM titanic\n",
    "GROUP BY Sex, Survived\n",
    "\"\"\"\n",
    "\n",
    "sns.barplot(x='Sex', y='cnt', hue='Survived', data=spark.sql(query).toPandas())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "남성의 사망자 수가 압도적으로 높다는 것을 알 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.탑승지 별 탑승수, 사망/생존자 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 탑승자 확인.\n",
    "query = \"\"\"\n",
    "SELECT Embarked, count(*) as cnt\n",
    "FROM titanic\n",
    "GROUP BY Embarked\n",
    "\"\"\"\n",
    "\n",
    "sns.barplot(x='Embarked', y='cnt', data=spark.sql(query).toPandas())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 탑승지 별 생존/사망 확인\n",
    "query = \"\"\"\n",
    "SELECT Embarked, Survived, count(*) as cnt\n",
    "FROM titanic\n",
    "GROUP BY Embarked, Survived\n",
    "\"\"\"\n",
    "\n",
    "sns.barplot(x='Embarked', y='cnt', hue='Survived', data=spark.sql(query).toPandas())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 요금 및 나이에 따른 생존자 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT Fare, Age, Survived\n",
    "FROM titanic\n",
    "\"\"\"\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "sns.scatterplot(x='Fare', y='Age', hue='Survived', data = spark.sql(query).toPandas())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "500 달러 이상을 낸 Outlier가 발견되었음. 제거하고 확인(200달러 미만)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT Fare, Age, Survived\n",
    "FROM titanic\n",
    "WHERE Fare < 200\n",
    "\"\"\"\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "sns.scatterplot(x='Fare', y='Age', hue='Survived', data = spark.sql(query).toPandas())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대체적으로 요금을 많이 내고, 나이가 어릴 수록 생존자가 많음을 확인 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 머신러닝 모델 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전처리 파이프라인 생성\n",
    "- 전처리 파이프라인 생성 전 null 값 여부를 먼저 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_sdf.select(\n",
    "    [ F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in titanic_sdf.columns ]\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cabin은 Section으로 대체 되었고, Embarked의 null 값은 `S`로 채우기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_sdf = titanic_sdf.fillna(\"S\", subset=[\"Embarked\"])\n",
    "titanic_sdf.select(\n",
    "    [ F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in titanic_sdf.columns ]\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 컬럼만 추출하기\n",
    "query=\"\"\"\n",
    "SELECT Survived,\n",
    "       Pclass,\n",
    "       Sex,\n",
    "       Age,\n",
    "       (Parch + Sibsp) as FamilySize,\n",
    "       Fare,\n",
    "       Embarked,\n",
    "       Section\n",
    "FROM titanic\n",
    "\"\"\"\n",
    "\n",
    "data_df = spark.sql(query)\n",
    "data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OneHotEncoding\n",
    "- Pclass, Sex, Embarked, Section은 OneHotEncoding 처리를 수행해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "cat_features = [\n",
    "    \"Pclass\"\n",
    "    \"Sex\",\n",
    "    \"Embarked\",\n",
    "    \"Section\"\n",
    "]\n",
    "\n",
    "for c in cat_features:\n",
    "    cat_indexer = StringIndexer(inputCol=c, outputCol=c+\"_idx\").setHandleInvalid(\"keep\")\n",
    "    onehot_encoder = OneHotEncoder(inputCols=[cat_indexer.getOutputCol()], outputCols=[c+\"_onehot\"])\n",
    "    stages += [cat_indexer, onehot_encoder]\n",
    "\n",
    "stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StandardScaling\n",
    "- FamilySize, Fare는 Standard Scaling 처리를 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "num_features = [\n",
    "    \"FamilySize\",\n",
    "    \"Fare\",\n",
    "]\n",
    "\n",
    "for n in num_features:\n",
    "    num_assembler = VectorAssembler(inputCols=[n], outputCol=n+\"_vector\")\n",
    "    num_scaler = StandardScaler(inputCol=num_assembler.getOutputCol(), outputCol=n+\"_scaled\")\n",
    "    \n",
    "    stages += [num_assembler, num_scaler]\n",
    "\n",
    "stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assemble_inputs = [c + \"_onehot\" for c in cat_features] + [n + \"_scaled\" for n in num_features]\n",
    "assemble_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_assembler = VectorAssembler(inputCols=assemble_inputs, outputCol='features')\n",
    "stages.append(total_assembler)\n",
    "stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련 / 테스트 데이터 세트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = data_df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파이프라인 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이프라인 등록\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=stages)\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파이프라인을 이용한 데이터 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit\n",
    "fitted_transformer = pipeline.fit(train_df)\n",
    "fitted_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_train_df = fitted_transformer.transform(train_df)\n",
    "vec_train_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    maxIter=50,\n",
    "    labelCol='Survived',\n",
    "    featuresCol='features',\n",
    "    regParam=0.1\n",
    ")\n",
    "\n",
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lr.fit(vec_train_df)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 테스트 데이터 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df 변환\n",
    "vec_test_df = fitted_transformer.transform(test_df)\n",
    "vec_test_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vec_test_df로 예측\n",
    "predictions = model.transform(vec_test_df)\n",
    "predictions.select(\"features\", \"Survived\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "    labelCol = 'Survived',\n",
    "    predictionCol = 'prediction',\n",
    "    metricName = 'accuracy'\n",
    ")\n",
    "\n",
    "accuracy = evaluator_accuracy.evaluate(predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
